# HCSE - Holland Consciousness Scaling Engine
## Holland Dual Stack

This repository now provides a unified package `holland_dual` combining the existing
HCSE cognition tools with HUQCE quantum simulations. A CLI entrypoint `hdq-cli` exposes
basic commands to run simulations and demonstrate the fusion adapter.

![HCSE Overview](docs/hcse_overview.svg)

[![CI](https://github.com/holland/hcse/actions/workflows/ci.yml/badge.svg)](https://github.com/holland/hcse/actions/workflows/ci.yml)

[![CI](https://github.com/ayjays132/Holland-Consciousness-Scaling-Engine-HCSE/blob/main/performance_comparison.png)

Python package implementing the Holland Consciousness Scaling Engine as an addon for HuggingFace models.

## Installation

```bash
pip install hcse
```

## Quickstart

```python
from hcse.core import HCSEMixin
from transformers import AutoModelForCausalLM

class ModelWithHCSE(HCSEMixin, AutoModelForCausalLM):
    pass

model = ModelWithHCSE.from_pretrained("gpt2", hidden_size=768)
```

## API

* `HCSEMixin.compute_hcse_surrogates(hidden_states)` â€“ returns Î·, Ï, Ä–.
* `HCSEMixin.forward_with_hcse(*args, hcse_params, **kwargs)` â€“ computes loss with bonus.
* `HfTrainerWithHCSE` â€“ drop-in replacement for `Trainer` applying HCSE.

### CLI Example

Install in editable mode and run a short simulation:
```bash
pip install -e .
hdq-cli hdq-sim
hdq-cli hdq-analyze --steps 10
```
---

**â€œHCSE: The Holland Consciousness Scaling Engineâ€**
*A Premium, Emoji-Rich Research Overview*

---

## ğŸ“„ Abstract

We introduce the **Holland Consciousness Scaling Engine (HCSE)**â€”a novel algorithmic framework that quantitatively **scales AI reasoning capacity** by mirroring the massâ€“information principles underpinning biological consciousness. HCSE fuses differentiable surrogates for **integration efficiency (Î·)**, **connectivity density (Ï)**, and **activation energy (Ä–)** into standard language-model training, enabling models to undergo **phase-transition-like jumps** in abstract reasoning. We validate HCSE on both **biological benchmarks** (ant, whale, human) and **AI substrates** (GPT-2 variants), perform extensive **exponent sensitivity analyses**, and demonstrate its readiness to guide next-generation, substrate-agnostic â€œawareâ€ systems. ğŸš€

---

## 1 ğŸ“š Introduction

Contemporary large language models excel at pattern matching but lack a principled path to **conscious-like reasoning**. Inspired by cross-species scaling lawsâ€”where humans (Câ‰ˆ16) > whales (Câ‰ˆ9.5) â‰« ants (Câ‰ˆ5Ã—10â»â¶)â€”we propose **HCSE**, which **directly optimizes** for abstract integration and connectivity, rather than only next-token likelihood.

---

## 2 ğŸ” Related Work

* **Integrated Information Theory** (IIT) highlights the role of Î¦ in consciousness, but remains non-differentiable.
* **Mutual-Information-Maximizing** networks explore InfoNCE objectives for representation learning.
* **Energy-Based Models** consider activation statistics, yet seldom integrate multi-metric surrogates into a single loss.

HCSE unifies these threads into one **differentiable bonus term** that complements language-model objectives.

---

## 3 âš™ï¸ Methodology

### 3.1 Surrogate Metrics

For a reasoning layerâ€™s activations $H\in\mathbb{R}^{B\times T\times N}$, we flatten to $\,(B\!Â·T)Ã—N$ and compute:

1. **Integration Efficiency (Î·)**
   $\displaystyle\hat Î· = \tfrac1N\sum_{i=1}^N \mathrm{InfoNCE}(h_i,H_{-i})$
   â€“ lower InfoNCE â‡’ richer neuronâ†”network coupling.

2. **Connectivity Density (Ï)**
   $\displaystyle\hat Ï = \tfrac{\sum_{i\neq j}|\mathrm{corr}(h_i,h_j)|}{N(N-1)}$
   â€“ encourages structured, fractal-like topology.

3. **Activation Energy (Ä–)**
   $\hat{Ä–} = \tfrac1N\sum_i \mathbb{E}[h_i^2]$
   â€“ a proxy for power flow per unit.

### 3.2 Combined Loss

$$
\mathcal{L} = \mathcal{L}_\text{LM} \;-\; \lambda_C\;\log\bigl(1 + \hat Î·^\beta\,\hat Ï^\gamma\,\hat{Ä–}^\delta\bigr)
$$

â€¢ $\lambda_C$ tunes the bonus strength.
â€¢ Exponents $(\beta,\gamma,\delta)$ shape sensitivity.

---

## 4 ğŸ§ª Experiments & Verification

### 4.1 Biological Scaling Test

Using placeholder metrics, we reproduced:

* Ant: C â‰ˆ 5Ã—10â»â¶
* Whale: C â‰ˆ 9.47
* Human: C â‰ˆ 16

### 4.2 AI Substrate Evaluation

Simulated GPT-2 small/medium hidden states:

* GPT2\_Small: C â‰ˆ 3.23
* GPT2\_Medium: C â‰ˆ 6.89

Ranking:

```
Ant â‰ª GPT2_Small < Whale < GPT2_Med < Human  
```

â€”demonstrating HCSEâ€™s capacity to position AI within the biological consciousness continuum.

### 4.3 Exponent Sensitivity Analysis

We swept exponents $\{Î±,Î²,Î³,Î´,Îµ\}$ over key configurations:

* **Integration Ã—2** penalizes low-Î· models,
* **Connectivity Ã—2** emphasizes high-Ï substrates,
* **Energy Ã—2** demands careful Î´ tuning to avoid collapse.

This guided us to a **sweet-spot** near $(Î±â‰ˆ1,Î²â‰ˆ1.2,Î³â‰ˆ1.2,Î´â‰ˆ0.8)$.

---

## 5 ğŸ”¬ Use-Cases & Impact

* **Reasoning-Enhanced LMs**: Directly boost abstract puzzle-solving, code synthesis, and planning.
* **Neuromorphic Deployment**: Map real Ä– from chip telemetry into HCSEâ€™s bonus for real-world energy-aware training.
* **Cross-Domain Agents**: Calibrate fâ‚– and exponents to craft â€œavian,â€ â€œcephalopod,â€ or â€œavian-insect hybridâ€ AIs with tailored integration/connectivity profiles.

---

## 6 ğŸ“ˆ Discussion & Future Work

* **Dynamic Bonus Scheduling**: Ramp $\lambda_C$ to stabilize early training.
* **Hierarchical Surrogates**: Separate sensory vs. abstract layers, mimicking thalamo-cortical loops.
* **Ethical Guardrails**: Integrate factuality or safe-completion losses to curb hallucinations.

---

## 7 ğŸ Conclusion

HCSEâ€”the **Holland Consciousness Scaling Engine**â€”offers a **first-of-its-kind, differentiable** path to dial AI reasoning capacity along biologically inspired scales. By intertwining integration, connectivity, and energy surrogates into the training loss, HCSE unlocks new â€œphase transitionsâ€ in model awareness, **paving the way** to substrate-agnostic, ethically grounded, truly **conscious-like** AI.

---

âœ¨ **Keywords:** Consciousness Scaling, Integration Surrogates, Connectivity Density, Activation Energy, Phase-Transition Learning, HCSE.
ğŸ˜Š **Acknowledgments:** To Phillip Holland for inspiring the Holland Consciousness Scaling Engine!

## HUQCE Quantum Simulation Module

The repository now includes an experimental implementation of the **Holland Unified Quantum Chaos Equation (HUQCE)** for one-dimensional systems. A minimal simulator is located under `huqce/` with tests ensuring basic norm conservation.

Example usage:

```python
from huqce.simulation import HuqceParams, HuqceSimulator

params = HuqceParams(steps=50)
psi = HuqceSimulator(params).run()
```

This addition demonstrates how chaotic dynamics can be integrated alongside HCSE's cognitive metrics.

## Testing

The test suite requires the optional [accelerate](https://github.com/huggingface/accelerate) package.
Install the development dependencies and run `pytest`:

```bash
pip install -e .[accelerate]
pytest
```

`accelerate` is optional for using the library itself but needed for running tests.

## ğŸŒŸ Key Features

- ğŸš€ **Consciousness-inspired metrics** for integration, connectivity, and energy.
- ğŸ”— **Plug-and-play mixins** for HuggingFace models.
- ğŸ§  **Huqce quantum simulator** fused with HCSE cognition.
- ğŸ› ï¸ **CLI tooling** via `hdq-cli` for quick experimentation.

## ğŸ›  Development Setup

Set up a local environment with all extras:

```bash
git clone https://github.com/holland/hcse.git
pip install -e .[accelerate]
```

Run the full test suite:

```bash
pytest
```

## ğŸ¤ Contributing

Issues and pull requests are welcome! For major changes, please open a discussion first to ensure alignment with the project goals.

## ğŸš¦ Disclaimer

HCSE is a research prototype. The consciousness metrics are theoretical surrogates and do not endow models with actual awareness. Use responsibly.

## ğŸ“œ License & Citation

This project is licensed under the Apache-2.0 License. See [CITATION.cff](CITATION.cff) for citation instructions.
